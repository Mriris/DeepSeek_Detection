import os
import json
import time
import random
import logging
import requests
import gzip
import concurrent.futures
from datetime import datetime
from tqdm import tqdm
import zipfile
import io
import pickle
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import psutil
import subprocess
import sys
import threading
import perfmonitor

# ==================== 配置参数 ====================
# 性能监控配置
PERFORMANCE_MONITOR = {
    "enabled": True,            # 是否启用性能监控
    "host": "127.0.0.1",        # 监听地址
    "port": 5001,               # 监听端口
    "update_interval": 1.0,     # 监控数据更新间隔（秒）
    "log_to_file": True,        # 是否将性能数据记录到文件
}

# 翻译配置
TRANSLATION = {
    "batch_size": 64,             # 批处理大小
    "max_length": 512,           # 最大文本长度
    "model_name": "Helsinki-NLP/opus-mt-en-zh",  # 翻译模型名称
    "timeout": 30,               # 翻译超时时间（秒）
    "fallback_timeout": 15,      # 回退策略超时时间（秒）
    "max_retries": 3,            # 最大重试次数
    "num_beams": 4,              # beam search宽度
    "use_gpu": True,             # 是否使用GPU
    "auto_reduce_batch": True,   # 自动减小批处理大小
    "enable_fallback": True      # 是否启用回退策略
}

# 数据处理配置
DATA_PROCESSING = {
    "sample_size": None,        # 要处理的样本数量，None表示处理全部
    "year_range": (datetime.now().year - 10, datetime.now().year)  # 漏洞数据的年份范围
}

# 配置日志记录
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("vulnerability_crawler.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("vulnerability_crawler")

# 数据目录
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
CVE_DIR = os.path.join(DATA_DIR, "cve")
OUTPUT_FILE = os.path.join(DATA_DIR, "cve_vulnerabilities.json")

# 确保目录存在
for directory in [DATA_DIR, CVE_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)

# 用户代理头轮换
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
]

# 检查并安装依赖
def check_dependencies():
    """
    检查并安装必要的依赖包
    
    Returns:
        bool: 是否所有依赖都已安装
    """
    required_packages = {
        "pynvml": "用于GPU性能监控",
        "psutil": "用于系统性能监控",
        "torch": "用于深度学习和GPU加速",
        "transformers": "用于文本翻译",
        "flask": "用于Web性能监控"
    }
    
    missing_packages = []
    
    for package, description in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append((package, description))
    
    if missing_packages:
        print("缺少以下依赖包:")
        for package, description in missing_packages:
            print(f"  - {package}: {description}")
        
        install = input("是否自动安装这些依赖包? (y/n): ")
        if install.lower() == 'y':
            for package, _ in missing_packages:
                print(f"正在安装 {package}...")
                try:
                    subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                    print(f"{package} 安装成功。")
                except Exception as e:
                    print(f"安装 {package} 失败: {str(e)}")
                    if package == "pynvml":
                        print("尝试安装 nvidia-ml-py3 作为替代...")
                        try:
                            subprocess.check_call([sys.executable, "-m", "pip", "install", "nvidia-ml-py3"])
                            print("nvidia-ml-py3 安装成功。")
                        except Exception as e2:
                            print(f"安装 nvidia-ml-py3 失败: {str(e2)}")
            
            # 再次检查是否所有依赖都已安装
            missing_after_install = []
            for package, description in missing_packages:
                try:
                    __import__(package)
                except ImportError:
                    missing_after_install.append(package)
            
            if missing_after_install:
                print(f"警告: 以下依赖包安装失败或未能正确加载: {', '.join(missing_after_install)}")
                print("程序将继续运行，但部分功能可能不可用。")
                return False
            else:
                print("所有依赖包已成功安装。")
                return True
        else:
            print("未安装依赖包，程序将继续运行，但部分功能可能不可用。")
            return False
    
    return True

# 获取随机请求头
def get_random_headers():
    """生成随机请求头"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Cache-Control": "max-age=0"
    }

# 文件下载函数
def download_file(url, save_path, chunk_size=8192):
    """
    下载文件并保存到指定路径
    
    Args:
        url: 文件URL
        save_path: 保存路径
        chunk_size: 每次读取的数据块大小
    
    Returns:
        bool: 是否下载成功
    """
    try:
        logger.info(f"开始下载: {url}")
        response = requests.get(url, headers=get_random_headers(), stream=True)
        response.raise_for_status()
        
        # 获取文件大小（如果可用）
        file_size = int(response.headers.get('Content-Length', 0))
        
        # 创建进度条
        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, desc=os.path.basename(save_path))
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=chunk_size):
                if chunk:
                    f.write(chunk)
                    progress_bar.update(len(chunk))
        
        progress_bar.close()
        logger.info(f"下载完成: {save_path}")
        return True
    except Exception as e:
        logger.error(f"下载 {url} 失败: {str(e)}")
        return False

def fetch_nvd_data(year_range=None):
    """
    获取NVD漏洞数据
    
    Args:
        year_range: 要获取的年份范围，例如 (2019, 2025)
    
    Returns:
        list: 漏洞数据列表
    """
    if year_range is None:
        # 使用全局配置的年份范围
        year_range = DATA_PROCESSING["year_range"]
    
    all_vulnerabilities = []
    
    # 获取历史数据
    for year in range(year_range[0], year_range[1] + 1):
        nvd_json_gz_url = f"https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-{year}.json.gz"
        nvd_json_gz_path = os.path.join(DATA_DIR, f"nvdcve-1.1-{year}.json.gz")
        
        if not os.path.exists(nvd_json_gz_path) or (datetime.now() - datetime.fromtimestamp(os.path.getmtime(nvd_json_gz_path))).days > 7:
            # 如果文件不存在或者文件超过7天未更新，则重新下载
            if not download_file(nvd_json_gz_url, nvd_json_gz_path):
                logger.warning(f"跳过 {year} 年数据处理")
                continue
        
        try:
            # 解析gzip压缩的JSON文件
            with gzip.open(nvd_json_gz_path, 'rt', encoding='utf-8') as f:
                nvd_data = json.load(f)
                cve_items = nvd_data.get('CVE_Items', [])
                
                for item in cve_items:
                    cve_id = item.get('cve', {}).get('CVE_data_meta', {}).get('ID', '')
                    published_date = item.get('publishedDate', '')
                    
                    # 解析描述信息
                    description = ""
                    desc_data = item.get('cve', {}).get('description', {}).get('description_data', [])
                    if desc_data:
                        description = desc_data[0].get('value', '')
                    
                    # 解析CVSS评分信息
                    cvss_v3 = item.get('impact', {}).get('baseMetricV3', {}).get('cvssV3', {})
                    cvss_v2 = item.get('impact', {}).get('baseMetricV2', {}).get('cvssV2', {})
                    
                    base_score_v3 = cvss_v3.get('baseScore', 0) if cvss_v3 else 0
                    base_score_v2 = cvss_v2.get('baseScore', 0) if cvss_v2 else 0
                    
                    # 确定风险等级
                    risk_level = "未知"
                    if base_score_v3 > 0:
                        if base_score_v3 >= 9.0:
                            risk_level = "高危"
                        elif base_score_v3 >= 7.0:
                            risk_level = "高危"
                        elif base_score_v3 >= 4.0:
                            risk_level = "中危"
                        else:
                            risk_level = "低危"
                    elif base_score_v2 > 0:
                        if base_score_v2 >= 7.0:
                            risk_level = "高危"
                        elif base_score_v2 >= 4.0:
                            risk_level = "中危"
                        else:
                            risk_level = "低危"
                    
                    # 解析漏洞类型
                    vuln_type = "未分类"
                    cwe_data = item.get('cve', {}).get('problemtype', {}).get('problemtype_data', [])
                    if cwe_data and cwe_data[0].get('description'):
                        for cwe in cwe_data[0].get('description', []):
                            if cwe.get('value', '').startswith('CWE-'):
                                vuln_type = cwe.get('value', '未分类')
                                break
                    
                    # 解析受影响的产品
                    affected_products = []
                    for node in item.get('configurations', {}).get('nodes', []):
                        if 'cpe_match' in node:
                            for cpe in node.get('cpe_match', []):
                                if cpe.get('vulnerable', False):
                                    cpe_parts = cpe.get('cpe23Uri', '').split(':')
                                    if len(cpe_parts) > 4:
                                        vendor = cpe_parts[3]
                                        product = cpe_parts[4]
                                        version = cpe_parts[5] if len(cpe_parts) > 5 else ""
                                        affected_products.append(f"{vendor} {product} {version}".strip())
                    
                    # 构建函数名（模拟）
                    function_name = f"function_{cve_id.lower().replace('-', '_')}"
                    
                    # 构建完整的漏洞信息
                    vulnerability = {
                        'id': cve_id,
                        'risk_level': risk_level,
                        'type': vuln_type,
                        'function': function_name,
                        'description': description,
                        'details': f"此漏洞的CVSS基础评分为：V3:{base_score_v3}, V2:{base_score_v2}。{description}",
                        'affected_versions': ", ".join(affected_products[:3]) + (", ..." if len(affected_products) > 3 else ""),
                        'cve_reference': f"https://cve.mitre.org/cgi-bin/cvename.cgi?name={cve_id}",
                        'discovered_date': published_date.split('T')[0] if 'T' in published_date else published_date,
                        'source': 'NVD'
                    }
                    
                    all_vulnerabilities.append(vulnerability)
                    
                logger.info(f"从 {year} 年的 NVD 数据中提取了 {len(cve_items)} 条漏洞记录")
                
                # 避免请求过于频繁
                time.sleep(random.uniform(0.5, 1.5))
                
        except Exception as e:
            logger.error(f"处理 {year} 年 NVD 数据时出错: {str(e)}")
    
    return all_vulnerabilities

def fetch_cve_mitre_data():
    """
    从 CVE MITRE 获取漏洞数据
    
    Returns:
        list: 漏洞数据列表
    """
    # CVE MITRE API
    cve_api_url = "https://cve.mitre.org/data/downloads/allitems.csv"
    cve_csv_path = os.path.join(CVE_DIR, "allitems.csv")
    
    all_vulnerabilities = []
    
    # 检查文件是否存在或者需要更新
    if not os.path.exists(cve_csv_path) or (datetime.now() - datetime.fromtimestamp(os.path.getmtime(cve_csv_path))).days > 7:
        # 如果文件不存在或者文件超过7天未更新，则重新下载
        if not download_file(cve_api_url, cve_csv_path):
            logger.warning("无法下载 CVE MITRE 数据，尝试使用缓存数据（如果有）")
    
    # 如果文件下载成功或者已经存在，则处理数据
    if os.path.exists(cve_csv_path):
        try:
            import csv
            with open(cve_csv_path, 'r', encoding='latin-1') as f:
                csv_reader = csv.reader(f)
                rows = list(csv_reader)
                
                # 跳过表头和注释行
                data_rows = []
                for row in rows:
                    if len(row) > 0 and row[0].startswith('CVE-'):
                        data_rows.append(row)
                
                # 使用进度条处理CSV数据
                for row in tqdm(data_rows, desc="处理 CVE MITRE 数据"):
                    try:
                        if len(row) >= 3:  # 确保至少有CVE ID和描述字段
                            cve_id = row[0]
                            status = row[1] if len(row) > 1 else ""
                            description = row[2] if len(row) > 2 else ""
                            
                            # 只处理已公开的CVE
                            if status.upper() != "REJECTED":
                                # 确定风险等级（基于简单规则）
                                risk_level = "中危"  # 默认为中危
                                
                                # 基于描述中的关键词判断风险等级
                                if any(kw in description.lower() for kw in ["remote code execution", "arbitrary code", "rce", "code execution", "重大"]):
                                    risk_level = "高危"
                                elif any(kw in description.lower() for kw in ["dos", "denial of service", "拒绝服务"]):
                                    risk_level = "中危"
                                elif any(kw in description.lower() for kw in ["information disclosure", "information leakage", "信息泄露"]):
                                    risk_level = "低危"
                                
                                # 确定漏洞类型（基于简单规则）
                                vuln_type = "未分类"
                                if "buffer" in description.lower() and ("overflow" in description.lower() or "overrun" in description.lower()):
                                    vuln_type = "缓冲区溢出"
                                elif "sql injection" in description.lower():
                                    vuln_type = "SQL注入"
                                elif "xss" in description.lower() or "cross site scripting" in description.lower():
                                    vuln_type = "跨站脚本"
                                elif "csrf" in description.lower() or "cross site request forgery" in description.lower():
                                    vuln_type = "跨站请求伪造"
                                elif "directory traversal" in description.lower() or "path traversal" in description.lower():
                                    vuln_type = "目录遍历"
                                elif "privilege" in description.lower() and "escalation" in description.lower():
                                    vuln_type = "权限提升"
                                
                                # 构建函数名（模拟）
                                function_name = f"function_{cve_id.lower().replace('-', '_')}"
                                
                                # 构建完整的漏洞信息
                                vulnerability = {
                                    'id': cve_id,
                                    'risk_level': risk_level,
                                    'type': vuln_type,
                                    'function': function_name,
                                    'description': description,
                                    'details': description,
                                    'affected_versions': "请参考CVE详情页面",
                                    'cve_reference': f"https://cve.mitre.org/cgi-bin/cvename.cgi?name={cve_id}",
                                    'discovered_date': "",  # MITRE CSV不包含日期信息
                                    'source': 'MITRE'
                                }
                                
                                all_vulnerabilities.append(vulnerability)
                    except Exception as e:
                        logger.error(f"处理 CVE MITRE 数据行时出错: {str(e)}")
                
                logger.info(f"从 CVE MITRE 数据中提取了 {len(all_vulnerabilities)} 条漏洞记录")
        except Exception as e:
            logger.error(f"处理 CVE MITRE 数据时出错: {str(e)}")
    
    return all_vulnerabilities

def translate_to_chinese(text, max_retries=3):
    """
    使用高效的本地翻译方法将文本翻译为中文
    
    Args:
        text: 要翻译的文本
        max_retries: 最大重试次数
    
    Returns:
        str: 翻译后的文本，如果翻译失败则返回原文本
    """
    if not text:
        return text
    
    try:
        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
        import torch
        
        # 使用@lru_cache装饰的内部函数确保模型只被加载一次
        @lru_cache(maxsize=1)
        def get_translator():
            # 使用更轻量级的模型
            model_name = "Helsinki-NLP/opus-mt-en-zh"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            # 如果有GPU，使用GPU加速
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = model.to(device)
            return tokenizer, model, device
        
        # 获取翻译器
        tokenizer, model, device = get_translator()
        
        # 限制文本长度以避免内存问题
        max_length = 512
        if len(text) > max_length:
            # 截断文本
            text = text[:max_length]
        
        # 翻译文本
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        # 将输入移动到GPU（如果可用）
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():  # 不计算梯度以提高速度
            try:
                # 添加超时控制
                translation_timeout = 15  # 设置为15秒
                
                # 使用更高效的生成参数
                translated = model.generate(
                    **inputs, 
                    max_length=512,
                    num_beams=2,  # 减小beam search宽度
                    length_penalty=1.0,  # 减少长句偏好
                    max_time=translation_timeout,  # 设置最大生成时间
                    early_stopping=True
                )
                result = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]
            except Exception as e:
                logger.warning(f"翻译生成超时或失败: {str(e)}")
                # 如果生成超时，尝试使用更简单的参数再次生成
                if "max_time" in str(e) or "timeout" in str(e).lower():
                    logger.info("尝试使用贪婪搜索重试翻译...")
                    try:
                        # 清理GPU缓存
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            
                        # 使用贪婪搜索（更快但质量可能降低）
                        translated = model.generate(
                            **inputs, 
                            max_length=256,  # 减少最大长度
                            num_beams=1,     # 贪婪搜索
                            do_sample=False,  # 不使用采样
                            max_time=10      # 更短的超时
                        )
                        result = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]
                    except Exception as inner_e:
                        logger.error(f"贪婪搜索翻译也失败: {str(inner_e)}")
                        return text  # 返回原文本
                else:
                    return text  # 其他错误返回原文本
        
        return result
            
    except Exception as e:
        logger.warning(f"翻译请求失败: {str(e)}")
        # 如果翻译失败，返回原文本
        return text

# GPU性能监控函数
def get_gpu_info():
    """
    获取GPU性能信息
    
    Returns:
        dict: GPU性能数据，包括使用率、内存使用量等
    """
    try:
        import pynvml
        import torch
        
        # 初始化NVML
        pynvml.nvmlInit()
        
        # 获取设备数量
        device_count = pynvml.nvmlDeviceGetCount()
        
        # 如果没有GPU，则返回空字典
        if device_count == 0 or not torch.cuda.is_available():
            return {"error": "没有可用的GPU"}
        
        # 获取GPU信息（默认使用第一个GPU）
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        
        # 获取GPU名称
        name = pynvml.nvmlDeviceGetName(handle)
        
        # 获取GPU使用率
        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
        gpu_util = utilization.gpu
        
        # 获取GPU内存信息
        memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
        total_memory = memory.total / (1024 ** 2)  # MB
        used_memory = memory.used / (1024 ** 2)    # MB
        memory_percent = (used_memory / total_memory) * 100
        
        # 获取GPU温度
        temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
        
        # 获取GPU功率使用情况
        power_usage = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W
        
        # 关闭NVML
        pynvml.nvmlShutdown()
        
        return {
            "name": name,
            "utilization": gpu_util,
            "memory_used_mb": used_memory,
            "memory_total_mb": total_memory,
            "memory_percent": memory_percent,
            "temperature": temperature,
            "power_usage": power_usage
        }
    except Exception as e:
        return {"error": f"获取GPU信息失败: {str(e)}"}

# 获取系统CPU和内存使用情况
def get_system_info():
    """
    获取系统CPU和内存使用情况
    
    Returns:
        dict: 系统性能数据
    """
    try:
        # 获取CPU使用率
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # 获取内存使用情况
        memory = psutil.virtual_memory()
        
        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_used_gb": memory.used / (1024 ** 3),
            "memory_total_gb": memory.total / (1024 ** 3)
        }
    except Exception as e:
        return {"error": f"获取系统信息失败: {str(e)}"}

# 性能监控器类
class PerformanceMonitor:
    """性能监控器，用于在后台线程中定期收集和显示性能数据"""
    
    def __init__(self, interval=1.0, log_file=None):
        """
        初始化性能监控器
        
        Args:
            interval: 监控间隔（秒）
            log_file: 性能日志文件路径，如果提供则记录性能数据
        """
        self.interval = interval
        self.running = False
        self.thread = None
        self.log_file = log_file
        self.start_time = time.time()
        self.perf_data = []  # 存储性能数据历史
        self.translation_stats = {
            "total_texts": 0,
            "processed_texts": 0,
            "avg_time_per_text": 0,
            "total_time": 0,
            "current_batch": 0,
            "total_batches": 0
        }
        
    def start(self):
        """启动性能监控"""
        if self.running:
            return
            
        self.running = True
        self.start_time = time.time()
        
        # 初始化性能日志文件
        if self.log_file:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write("timestamp,gpu_util,gpu_mem_used,gpu_mem_total,gpu_mem_percent,gpu_temp,gpu_power,cpu_percent,mem_used_gb,mem_total_gb,mem_percent,texts_processed,avg_time_per_text\n")
        
        # 创建监控线程
        import threading
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.daemon = True  # 设置为守护线程，主线程结束时自动终止
        self.thread.start()
        
    def stop(self):
        """停止性能监控"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)  # 最多等待2秒
            
        # 生成性能报告
        if len(self.perf_data) > 0:
            self._generate_performance_report()
            
    def update_translation_stats(self, stats):
        """更新翻译统计信息"""
        self.translation_stats.update(stats)
            
    def _monitor_loop(self):
        """监控循环，定期收集和显示性能数据"""
        import time
        
        while self.running:
            # 收集GPU信息
            gpu_info = get_gpu_info()
            
            # 收集系统信息
            sys_info = get_system_info()
            
            # 收集性能数据
            perf_data = {
                "timestamp": time.time() - self.start_time,
                "gpu_info": gpu_info,
                "sys_info": sys_info,
                "translation_stats": self.translation_stats.copy()
            }
            self.perf_data.append(perf_data)
            
            # 显示性能数据
            self._display_performance(gpu_info, sys_info)
            
            # 记录性能数据到日志文件
            if self.log_file:
                self._log_performance(perf_data)
            
            # 等待下一个监控周期
            time.sleep(self.interval)
    
    def _display_performance(self, gpu_info, sys_info):
        """显示性能数据"""
        # 清空当前行
        print("\r", end="")
        
        # 显示GPU信息
        if "error" in gpu_info:
            gpu_str = "GPU: 不可用"
        else:
            gpu_str = f"GPU: {gpu_info['name']}, 使用率: {gpu_info['utilization']}%, 内存: {gpu_info['memory_used_mb']:.0f}/{gpu_info['memory_total_mb']:.0f} MB ({gpu_info['memory_percent']:.1f}%), 温度: {gpu_info['temperature']}°C, 功率: {gpu_info['power_usage']:.1f}W"
        
        # 显示系统信息
        if "error" in sys_info:
            sys_str = "系统: 信息获取失败"
        else:
            sys_str = f"CPU: {sys_info['cpu_percent']}%, 内存: {sys_info['memory_used_gb']:.1f}/{sys_info['memory_total_gb']:.1f} GB ({sys_info['memory_percent']}%)"
        
        # 显示翻译进度
        progress_str = ""
        if self.translation_stats["total_texts"] > 0:
            progress = self.translation_stats["processed_texts"] / self.translation_stats["total_texts"] * 100
            elapsed = time.time() - self.start_time
            
            if self.translation_stats["processed_texts"] > 0:
                avg_time = self.translation_stats["total_time"] / self.translation_stats["processed_texts"]
                eta = (self.translation_stats["total_texts"] - self.translation_stats["processed_texts"]) * avg_time
                
                progress_str = f" | 进度: {progress:.1f}% ({self.translation_stats['processed_texts']}/{self.translation_stats['total_texts']}), 批次: {self.translation_stats['current_batch']}/{self.translation_stats['total_batches']}, 平均: {avg_time:.3f}秒/条, 剩余: {eta/60:.1f}分钟"
        
        # 输出完整性能信息
        print(f"[性能监控] {gpu_str} | {sys_str}{progress_str}", end="")
    
    def _log_performance(self, perf_data):
        """记录性能数据到日志文件"""
        if not self.log_file:
            return
            
        try:
            gpu_info = perf_data["gpu_info"]
            sys_info = perf_data["sys_info"]
            translation_stats = perf_data["translation_stats"]
            
            if "error" not in gpu_info and "error" not in sys_info:
                with open(self.log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{perf_data['timestamp']:.2f},{gpu_info['utilization']},{gpu_info['memory_used_mb']:.1f},{gpu_info['memory_total_mb']:.1f},{gpu_info['memory_percent']:.1f},{gpu_info['temperature']},{gpu_info['power_usage']:.2f},{sys_info['cpu_percent']},{sys_info['memory_used_gb']:.2f},{sys_info['memory_total_gb']:.2f},{sys_info['memory_percent']},{translation_stats['processed_texts']},{translation_stats['avg_time_per_text']:.4f}\n")
        except Exception as e:
            # 如果记录失败，不应该影响主程序运行
            pass
    
    def _generate_performance_report(self):
        """生成性能报告"""
        if len(self.perf_data) == 0:
            return
            
        logger.info("正在生成性能报告...")
        
        try:
            # 计算平均性能指标
            valid_gpu_data = [p for p in self.perf_data if "error" not in p["gpu_info"]]
            valid_sys_data = [p for p in self.perf_data if "error" not in p["sys_info"]]
            
            if valid_gpu_data:
                avg_gpu_util = sum(p["gpu_info"]["utilization"] for p in valid_gpu_data) / len(valid_gpu_data)
                avg_gpu_mem = sum(p["gpu_info"]["memory_percent"] for p in valid_gpu_data) / len(valid_gpu_data)
                avg_gpu_temp = sum(p["gpu_info"]["temperature"] for p in valid_gpu_data) / len(valid_gpu_data)
                max_gpu_util = max(p["gpu_info"]["utilization"] for p in valid_gpu_data)
                max_gpu_mem = max(p["gpu_info"]["memory_percent"] for p in valid_gpu_data)
                
                logger.info(f"GPU平均使用率: {avg_gpu_util:.1f}%, 最大: {max_gpu_util}%")
                logger.info(f"GPU平均内存占用: {avg_gpu_mem:.1f}%, 最大: {max_gpu_mem:.1f}%")
                logger.info(f"GPU平均温度: {avg_gpu_temp:.1f}°C")
            
            if valid_sys_data:
                avg_cpu_util = sum(p["sys_info"]["cpu_percent"] for p in valid_sys_data) / len(valid_sys_data)
                avg_mem_util = sum(p["sys_info"]["memory_percent"] for p in valid_sys_data) / len(valid_sys_data)
                
                logger.info(f"CPU平均使用率: {avg_cpu_util:.1f}%")
                logger.info(f"内存平均使用率: {avg_mem_util:.1f}%")
            
            # 输出翻译性能统计
            final_stats = self.perf_data[-1]["translation_stats"]
            if final_stats["processed_texts"] > 0:
                total_time = final_stats["total_time"]
                texts_per_second = final_stats["processed_texts"] / total_time if total_time > 0 else 0
                logger.info(f"翻译性能: {final_stats['processed_texts']}条文本, 总时间: {total_time:.1f}秒, 速度: {texts_per_second:.2f}条/秒")
            
            # 如果有性能日志文件，提供绘图分析建议
            if self.log_file and os.path.exists(self.log_file):
                logger.info(f"详细性能数据已保存到: {self.log_file}")
                logger.info("可以使用以下Python代码绘制性能曲线图：")
                logger.info("""
import pandas as pd
import matplotlib.pyplot as plt

# 读取性能日志
perf_data = pd.read_csv("翻译性能日志文件路径.csv")

# 绘制GPU使用率曲线
plt.figure(figsize=(12, 6))
plt.plot(perf_data['timestamp'], perf_data['gpu_util'], label='GPU使用率(%)')
plt.plot(perf_data['timestamp'], perf_data['gpu_mem_percent'], label='GPU内存占用(%)')
plt.xlabel('时间(秒)')
plt.ylabel('百分比(%)')
plt.title('GPU性能监控')
plt.legend()
plt.grid(True)
plt.savefig('gpu_performance.png')
plt.show()
                """)
        except Exception as e:
            logger.error(f"生成性能报告时出错: {str(e)}")

# 修改批量翻译函数以使用Web性能监控
def batch_translate_to_chinese(texts, batch_size=None, progress_callback=None, global_start_index=0):
    """
    批量翻译文本以提高效率，利用GPU并行处理，并通过Web界面显示实时性能开销
    
    Args:
        texts: 要翻译的文本列表
        batch_size: 批处理大小，如果为None则使用全局配置
        progress_callback: 进度回调函数，接收处理的文本数量和处理时间
        global_start_index: 全局文本起始索引，用于计算总体进度
    
    Returns:
        list: 翻译后的文本列表
    """
    if not texts:
        return []
    
    # 使用全局配置参数（如果未指定）
    if batch_size is None:
        batch_size = TRANSLATION["batch_size"]
    
    max_length = TRANSLATION["max_length"]
    model_name = TRANSLATION["model_name"]
    timeout = TRANSLATION["timeout"]
    num_beams = TRANSLATION["num_beams"]
    auto_reduce_batch = TRANSLATION["auto_reduce_batch"]
    
    # 初始化自适应批处理大小控制
    adaptive_batch_size = batch_size
    min_batch_size = 1
    
    # 跟踪小批次进度
    current_processed_count = 0
    current_total_time = 0
    
    try:
        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
        import torch
        
        # 本地翻译批次信息 - 仅用于本地显示进度，不影响全局进度
        total_texts = len(texts)
        num_batches = (total_texts + adaptive_batch_size - 1) // adaptive_batch_size
        
        # 注意：不再直接更新全局翻译统计信息，改用局部变量跟踪进度
        local_stats = {
            "total_texts": total_texts,
            "processed_texts": 0,
            "total_batches": num_batches,
            "current_batch": 0
        }
        
        @lru_cache(maxsize=1)
        def get_translator():
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            # 使用GPU加速（如果可用）
            device = "cuda" if torch.cuda.is_available() and TRANSLATION["use_gpu"] else "cpu"
            model = model.to(device)
            logger.info(f"使用 {device} 进行翻译，模型: {model_name}")
            return tokenizer, model, device
        
        tokenizer, model, device = get_translator()
        
        # 获取并显示初始GPU信息
        gpu_info = get_gpu_info()
        if "error" not in gpu_info:
            logger.info(f"GPU: {gpu_info['name']}, 总内存: {gpu_info['memory_total_mb']:.0f}MB")
        
        # 处理每个批次
        results = []
        processed_texts = 0
        total_time = 0
        failures = 0  # 跟踪失败次数
        
        # 创建一个带有动态描述的进度条
        pbar = tqdm(range(0, len(texts), adaptive_batch_size), total=num_batches, desc="批量翻译")
        
        for i in range(0, len(texts), adaptive_batch_size):
            # 更新当前局部批次
            local_stats["current_batch"] += 1
            
            # 先检查内存情况
            if check_and_free_memory(threshold_percent=85, gpu_threshold_percent=85):
                # 如果内存紧张，减小批处理大小
                if auto_reduce_batch and adaptive_batch_size > min_batch_size:
                    old_batch_size = adaptive_batch_size
                    adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
                    logger.warning(f"由于内存压力，批处理大小从 {old_batch_size} 减小到 {adaptive_batch_size}")
                    # 更新总批次数
                    num_batches = (len(texts) - processed_texts + adaptive_batch_size - 1) // adaptive_batch_size
                    pbar.total = num_batches
                    pbar.refresh()
                    
                    # 更新局部批次信息
                    local_stats["total_batches"] = num_batches
                    
                    # 额外休息以确保内存和GPU得到充分释放
                    time.sleep(3)
            
            batch_start_time = time.time()
            
            # 获取当前批次文本
            end_idx = min(i + adaptive_batch_size, len(texts))
            batch_texts = texts[i:end_idx]
            
            # 过滤掉空文本并记录位置
            valid_indices = [j for j, text in enumerate(batch_texts) if text]
            if not valid_indices:
                results.extend([""] * len(batch_texts))
                # 空文本也视为处理完成
                processed_texts += len(batch_texts)
                current_processed_count += len(batch_texts)
                local_stats["processed_texts"] += len(batch_texts)
                # 回调进度
                if progress_callback:
                    progress_callback(len(batch_texts), 0)
                continue
                
            valid_texts = [batch_texts[j] for j in valid_indices]
            
            # 截断长文本
            valid_texts = [text[:max_length] for text in valid_texts]
            
            try:
                # 批量编码并翻译
                encoded = tokenizer(valid_texts, padding=True, truncation=True, 
                                  return_tensors="pt", max_length=max_length)
                # 将输入移动到GPU（如果可用）
                encoded = {k: v.to(device) for k, v in encoded.items()}
                
                # 获取编码后的GPU使用情况
                gpu_info_encoded = get_gpu_info()
                
                # 更新进度条描述
                if "error" not in gpu_info_encoded and device == "cuda":
                    memory_info = f"GPU内存: {gpu_info_encoded['memory_used_mb']:.0f}MB/{gpu_info_encoded['memory_total_mb']:.0f}MB ({gpu_info_encoded['memory_percent']:.1f}%)"
                    progress_info = f"进度: {local_stats['processed_texts']}/{local_stats['total_texts']} ({local_stats['current_batch']}/{local_stats['total_batches']}批)"
                    pbar.set_description(f"批量翻译 [{memory_info}] [{progress_info}] 批大小: {adaptive_batch_size}")
                
                with torch.no_grad():
                    try:
                        # 记录处理的每个文本，并更新性能监控
                        for j, source_text in enumerate(valid_texts):
                            # 更新当前翻译内容（开始翻译）
                            perfmonitor.update_current_translation(
                                source_text=source_text,
                                batch_index=local_stats["current_batch"], 
                                text_index=j+1
                            )
                        
                        # 翻译开始时间
                        translation_start = time.time()
                        
                        # 生成翻译，添加超时控制
                        try:
                            # 使用timeout进行生成，防止模型生成过程卡住
                            translated = model.generate(
                                **encoded, 
                                max_length=max_length,
                                num_beams=num_beams,  # 使用配置的beam search宽度
                                length_penalty=1.0,  # 降低长句生成倾向，减少内存消耗
                                max_time=timeout,  # 最大生成时间（秒）
                                early_stopping=True
                            )
                        except Exception as e:
                            logger.warning(f"翻译生成超时或错误: {str(e)}，尝试分批处理")
                            # 失败时，尝试将当前批次拆分为更小批次重试
                            if len(valid_texts) > 1:
                                mid = len(valid_texts) // 2
                                logger.info(f"将当前批次拆分为两部分：{mid}条和{len(valid_texts)-mid}条")
                                
                                # 递归处理前半部分
                                first_half_results = batch_translate_to_chinese(
                                    valid_texts[:mid], 
                                    batch_size=max(1, adaptive_batch_size//2),
                                    progress_callback=progress_callback,
                                    global_start_index=global_start_index + current_processed_count
                                )
                                # 递归处理后半部分
                                second_half_results = batch_translate_to_chinese(
                                    valid_texts[mid:], 
                                    batch_size=max(1, adaptive_batch_size//2),
                                    progress_callback=progress_callback,
                                    global_start_index=global_start_index + current_processed_count + mid
                                )
                                
                                # 合并结果
                                decoded = first_half_results + second_half_results
                                
                                # 清理GPU缓存
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                                    
                                # 计算翻译时间
                                translation_time = time.time() - translation_start
                                
                                # 更新当前批次的处理计数
                                current_processed_count += len(valid_texts)
                                local_stats["processed_texts"] += len(valid_texts)
                                
                                # 如果分批处理成功，减小下一次的批处理大小
                                if auto_reduce_batch and adaptive_batch_size > min_batch_size:
                                    adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
                                    logger.info(f"分批处理成功，将批处理大小减小到 {adaptive_batch_size}")
                                    # 更新批次总数
                                    remaining_texts = len(texts) - (i + len(batch_texts))
                                    remaining_batches = (remaining_texts + adaptive_batch_size - 1) // adaptive_batch_size
                                    local_stats["total_batches"] = local_stats["current_batch"] + remaining_batches
                                    pbar.total = local_stats["total_batches"]
                                    pbar.refresh()
                            else:
                                # 单条文本处理失败，使用原文本
                                logger.warning(f"单条文本翻译失败，使用原文本: {valid_texts[0][:30]}...")
                                decoded = valid_texts
                                translation_time = time.time() - translation_start
                                failures += 1
                                
                                # 更新当前批次的处理计数
                                current_processed_count += len(valid_texts)
                                local_stats["processed_texts"] += len(valid_texts)
                                
                                # 回调进度
                                if progress_callback:
                                    progress_callback(len(valid_texts), translation_time)
                                
                                # 如果持续失败，可能需要进一步降低批处理大小或参数
                                if failures >= 3 and auto_reduce_batch:
                                    num_beams = max(1, num_beams - 1)
                                    max_length = max(128, max_length // 2)
                                    logger.warning(f"多次失败，降低参数: num_beams={num_beams}, max_length={max_length}")
                                    failures = 0  # 重置失败计数
                        else:
                            # 翻译正常完成，解码结果
                            decoded = tokenizer.batch_decode(translated, skip_special_tokens=True)
                            # 翻译结束时间
                            translation_end = time.time()
                            translation_time = translation_end - translation_start
                            # 重置失败计数
                            failures = 0
                            
                            # 更新当前批次的处理计数
                            current_processed_count += len(valid_texts)
                            current_total_time += translation_time
                            local_stats["processed_texts"] += len(valid_texts)
                            
                            # 回调进度 - 只有通过回调才会更新全局进度
                            if progress_callback:
                                progress_callback(len(valid_texts), translation_time)
                            
                            # 尝试逐渐增加批处理大小（如果之前有减小）
                            if adaptive_batch_size < batch_size and auto_reduce_batch:
                                adaptive_batch_size = min(batch_size, adaptive_batch_size * 2)
                                logger.info(f"翻译成功，尝试将批处理大小增加到 {adaptive_batch_size}")
                        
                        # 获取翻译后的GPU使用情况
                        gpu_info_after = get_gpu_info()
                        
                        # 重建完整的结果列表（包括空文本的位置）
                        batch_results = [""] * len(batch_texts)
                        for j, idx in enumerate(valid_indices):
                            if j < len(decoded):  # 防止索引越界
                                batch_results[idx] = decoded[j]
                            
                            # 更新当前翻译内容（完成翻译，显示结果）
                            if j < len(valid_texts) and j < len(decoded):
                                source_text = valid_texts[j]
                                translated_text = decoded[j] if j < len(decoded) else ""
                                # 计算单个文本的平均处理时间
                                text_time = translation_time / len(valid_texts)
                                
                                # 更新当前翻译内容
                                perfmonitor.update_current_translation(
                                    source_text=source_text,
                                    translated_text=translated_text,
                                    processing_time=text_time,
                                    batch_index=local_stats["current_batch"],
                                    text_index=j+1
                                )
                                # 短暂停顿以显示当前翻译内容
                                time.sleep(0.05)  # 减少暂停时间，加快处理速度
                        
                        results.extend(batch_results)
                        
                        # 更新性能统计信息 - 仅局部统计
                        batch_texts_count = len(valid_texts)
                        processed_texts += batch_texts_count
                        total_time += translation_time
                        avg_time_per_text = total_time / processed_texts if processed_texts > 0 else 0
                        
                        # 显示当前批次翻译性能
                        if "error" not in gpu_info_after and device == "cuda":
                            texts_per_second = batch_texts_count / translation_time if translation_time > 0 else 0
                            logger.debug(f"批次 {local_stats['current_batch']}/{local_stats['total_batches']}: {batch_texts_count}条文本, {translation_time:.2f}秒, {texts_per_second:.1f}条/秒, GPU内存: {gpu_info_after['memory_used_mb']:.0f}MB")
                            
                    except Exception as e:
                        logger.error(f"批量翻译失败: {str(e)}")
                        # 失败时返回原文本
                        results.extend(batch_texts)
                        failures += 1
                        
                        # 更新当前批次的处理计数
                        current_processed_count += len(batch_texts)
                        local_stats["processed_texts"] += len(batch_texts)
                        
                        # 回调进度（处理失败也算作处理完成）
                        if progress_callback:
                            progress_callback(len(batch_texts), 0)
                        
                        # 如果失败，强制清理内存并减小批处理大小
                        check_and_free_memory(threshold_percent=0, gpu_threshold_percent=0)  # 强制清理
                        
                        if auto_reduce_batch and adaptive_batch_size > min_batch_size:
                            adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
                            logger.warning(f"翻译失败，将批处理大小减小到 {adaptive_batch_size}")
                
                # 计算批次总时间
                batch_time = time.time() - batch_start_time
                
                # 短暂休眠以防止GPU过热
                # 动态调整休眠时间，根据批次处理时间，避免过长休眠
                sleep_time = min(0.5, max(0.05, batch_time * 0.05))
                time.sleep(sleep_time)
            
            except Exception as batch_e:
                logger.error(f"处理批次时出错: {str(batch_e)}")
                # 添加当前批次的原始文本，确保结果大小一致
                results.extend(batch_texts)
                
                # 更新当前批次的处理计数
                current_processed_count += len(batch_texts)
                local_stats["processed_texts"] += len(batch_texts)
                
                # 回调进度（处理失败也算作处理完成）
                if progress_callback:
                    progress_callback(len(batch_texts), 0)
                
                # 强制清理内存
                check_and_free_memory(threshold_percent=0, gpu_threshold_percent=0)
                time.sleep(2)  # 给系统恢复的时间
                
                # 降低批处理大小
                if auto_reduce_batch and adaptive_batch_size > min_batch_size:
                    adaptive_batch_size = max(min_batch_size, adaptive_batch_size // 2)
                    logger.warning(f"批处理失败，将批处理大小减小到 {adaptive_batch_size}")
            
            # 每处理完5个批次强制清理一次内存
            if (local_stats["current_batch"] % 5) == 0:
                check_and_free_memory()
                
            # 更新进度条
            pbar.update(1)
        
        pbar.close()
        return results
    
    except Exception as e:
        logger.error(f"批量翻译初始化失败: {str(e)}")
        # 回退到单个翻译
        single_results = []
        for text in tqdm(texts, desc="单个翻译"):
            result = translate_to_chinese(text)
            single_results.append(result)
            # 更新当前批次的处理计数
            current_processed_count += 1
            # 回调进度
            if progress_callback:
                progress_callback(1, 0)
        return single_results

# 修改enrich_data_with_chinese_translation函数使用全局配置
def enrich_data_with_chinese_translation(vulnerabilities, sample_size=None):
    """
    使用中文翻译丰富漏洞数据，采用高效并行处理提高速度
    
    Args:
        vulnerabilities: 漏洞数据列表
        sample_size: 要翻译的样本数量，默认为None表示翻译所有数据
    
    Returns:
        list: 丰富后的漏洞数据列表
    """
    # 如果未指定sample_size，使用全局配置
    if sample_size is None:
        sample_size = DATA_PROCESSING["sample_size"]
    
    # 如果指定了样本大小，则随机选择样本；否则处理所有数据
    if sample_size and len(vulnerabilities) > sample_size:
        sample_indices = random.sample(range(len(vulnerabilities)), sample_size)
        samples = [vulnerabilities[i] for i in sample_indices]
    else:
        samples = vulnerabilities[:]
    
    logger.info(f"开始为 {len(samples)} 条漏洞数据添加中文翻译")
    
    # 收集所有需要翻译的文本
    all_texts_to_translate = []
    text_locations = []  # 存储每个文本在原数据中的位置信息
    
    for vuln_idx, vuln in enumerate(samples):
        # 收集描述文本
        if 'description' in vuln and vuln['description']:
            all_texts_to_translate.append(vuln['description'])
            text_locations.append((vuln_idx, 'description_zh'))
        
        # 收集详情文本
        if 'details' in vuln and vuln['details']:
            all_texts_to_translate.append(vuln['details'])
            text_locations.append((vuln_idx, 'details_zh'))
        
        # 收集漏洞类型
        if 'type' in vuln and vuln['type'] and vuln['type'] != '未分类' and not vuln['type'].startswith('CWE-'):
            all_texts_to_translate.append(vuln['type'])
            text_locations.append((vuln_idx, 'type_zh'))
        
        # 收集受影响版本
        if 'affected_versions' in vuln and vuln['affected_versions']:
            all_texts_to_translate.append(vuln['affected_versions'])
            text_locations.append((vuln_idx, 'affected_versions_zh'))
    
    logger.info(f"共收集到 {len(all_texts_to_translate)} 条需要翻译的文本")
    
    # 更新全局进度信息 - 总文本数
    total_texts_to_translate = len(all_texts_to_translate)
    start_time = time.time()
    processed_texts_global = 0
    
    # 初始化全局翻译统计信息
    perfmonitor.update_translation_stats({
        "total_texts": total_texts_to_translate,
        "processed_texts": 0,
        "avg_time_per_text": 0,
        "total_time": 0,
        "current_batch": 0,
        "total_batches": 0
    })
    
    # 优化GPU批处理大小
    import torch
    batch_size = TRANSLATION["batch_size"]  # 使用全局配置的批处理大小
    if torch.cuda.is_available():
        # 根据GPU显存动态调整批处理大小
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # 显存大小(GB)
        if gpu_mem >= 15:
            pass
        elif gpu_mem >= 7:
            batch_size = min(32, batch_size)
        elif gpu_mem >= 3:
            batch_size = min(16, batch_size)
        logger.info(f"检测到GPU显存 {gpu_mem:.1f}GB，设置批处理大小为 {batch_size}")
    
    # 批量翻译所有文本
    logger.info("开始批量翻译...")
    all_translated_texts = []
    
    # 分多个大批次处理，减小批次大小以避免内存问题
    large_batch_size = 500  # 从2000减小到500
    # 计算总批次数
    total_large_batches = (len(all_texts_to_translate) + large_batch_size - 1) // large_batch_size
    
    # 单独更新批次信息，与进度信息分离
    perfmonitor.update_translation_stats({
        "current_batch": 0,
        "total_batches": total_large_batches
    })
    
    # 初始化或重置进度信息
    perfmonitor.update_translation_stats({
        "total_texts": total_texts_to_translate,
        "processed_texts": 0,
        "avg_time_per_text": 0,
        "total_time": 0,
        "remaining_time": 0,
        "progress_percent": 0
    })
    
    for batch_idx, start_idx in enumerate(range(0, len(all_texts_to_translate), large_batch_size)):
        end_idx = min(start_idx + large_batch_size, len(all_texts_to_translate))
        current_batch = all_texts_to_translate[start_idx:end_idx]
        current_locations = text_locations[start_idx:end_idx]
        
        # 更新当前处理的大批次 - 单独更新批次信息，避免干扰进度信息
        current_large_batch = batch_idx + 1
        perfmonitor.update_translation_stats({
            "current_batch": current_large_batch,
            "total_batches": total_large_batches
        })
        
        logger.info(f"正在翻译第 {current_large_batch}/{total_large_batches} 大批次，共 {len(current_batch)} 条文本")
        
        try:
            # 不再在此处更新翻译进度信息
            
            # 检查缓存
            cached_results = translation_cache.get_batch(current_batch)
            need_translate_indices = []
            need_translate_texts = []
            
            for i, (text, cached) in enumerate(zip(current_batch, cached_results)):
                if not cached and text:
                    need_translate_indices.append(i)
                    need_translate_texts.append(text)
            
            # 翻译未缓存的文本
            if need_translate_texts:
                logger.info(f"需要翻译 {len(need_translate_texts)} 条未缓存文本")
                
                # 减小翻译批处理大小，并使用自动调整的批处理大小
                adjusted_batch_size = batch_size
                logger.info(f"批处理大小: {adjusted_batch_size}")
                
                # 当前大批次中未缓存文本开始的总体索引
                batch_start_index = processed_texts_global + len(current_batch) - len(need_translate_texts)
                
                # 定义翻译进度回调函数
                def translation_progress_callback(processed_count, batch_time):
                    nonlocal processed_texts_global
                    # 更新已处理文本总数
                    new_processed_texts = processed_texts_global + processed_count
                    processed_texts_global = new_processed_texts
                    
                    # 计算平均处理时间和预计剩余时间
                    elapsed_time = time.time() - start_time
                    texts_per_second = new_processed_texts / elapsed_time if elapsed_time > 0 else 0
                    estimated_total_time = total_texts_to_translate / texts_per_second if texts_per_second > 0 else 0
                    remaining_time = max(0, estimated_total_time - elapsed_time)
                    
                    # 计算总体进度百分比
                    progress_percent = (new_processed_texts / total_texts_to_translate * 100) if total_texts_to_translate > 0 else 0
                    
                    # 仅更新全局统计数据，确保专门更新进度数据
                    perfmonitor.update_translation_stats({
                        "processed_texts": new_processed_texts,
                        "total_texts": total_texts_to_translate,
                        "avg_time_per_text": elapsed_time / new_processed_texts if new_processed_texts > 0 else 0,
                        "total_time": elapsed_time,
                        "remaining_time": remaining_time,
                        "progress_percent": progress_percent
                        # 注意：不更新批次信息，避免覆盖
                    })
                
                try:
                    # 传入回调函数和起始索引
                    translated_texts = batch_translate_to_chinese(
                        need_translate_texts, 
                        batch_size=adjusted_batch_size,
                        progress_callback=translation_progress_callback,
                        global_start_index=batch_start_index
                    )
                    
                    # 更新已处理文本计数
                    processed_texts_global += len(need_translate_texts)
                    
                except Exception as e:
                    # 翻译失败时的应急处理
                    logger.error(f"批量翻译失败: {str(e)}，尝试单条翻译")
                    translated_texts = []
                    
                    # 使用单条翻译处理（可能很慢，但更稳定）
                    for text_idx, text in enumerate(tqdm(need_translate_texts, desc="单条翻译")):
                        try:
                            # 清理GPU缓存
                            if 'torch' in sys.modules and torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            
                            translated = translate_to_chinese(text)
                            translated_texts.append(translated)
                            
                            # 逐条更新进度
                            processed_texts_global += 1
                            translation_progress_callback(1, 0)
                            
                        except Exception as inner_e:
                            logger.error(f"单条翻译失败: {str(inner_e)}")
                            translated_texts.append(text)  # 失败时使用原文
                            processed_texts_global += 1
                            translation_progress_callback(1, 0)
                
                # 更新缓存
                for i, translated in enumerate(translated_texts):
                    if translated != need_translate_texts[i]:  # 只缓存真正翻译了的结果
                        translation_cache.put(need_translate_texts[i], translated)
                
                # 合并结果
                batch_results = list(cached_results)  # 创建副本
                for i, trans_idx in enumerate(need_translate_indices):
                    if i < len(translated_texts):  # 防止索引越界
                        batch_results[trans_idx] = translated_texts[i]
                    else:
                        batch_results[trans_idx] = current_batch[trans_idx]  # 使用原文
            else:
                # 缓存命中，直接使用缓存结果
                batch_results = cached_results
                
                # 更新已处理文本计数（已缓存的文本也算作已处理）
                processed_texts_global += len(current_batch)
                
                # 通过与其他进度更新相同的回调机制更新缓存命中的进度
                translation_progress_callback(len(current_batch), 0)
            
            # 应用翻译结果到原始数据
            for i, (vuln_idx, field_name) in enumerate(current_locations):
                if i < len(batch_results):  # 防止索引越界
                    result = batch_results[i] if batch_results[i] else current_batch[i]
                    samples[vuln_idx][field_name] = result
            
            # 保存翻译缓存（每批次都保存，确保中断时不会丢失缓存）
            translation_cache.save_cache(force=True)
            
            # 合并结果
            all_translated_texts.extend(batch_results)
                
        except Exception as e:
            logger.error(f"处理批次 {current_large_batch}/{total_large_batches} 失败: {str(e)}")
            # 跳过这个批次，确保程序不会完全中断
            continue
            
        # 每个大批次处理完后，强制清理GPU内存
        if 'torch' in sys.modules and torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("已清理GPU缓存")
            # 短暂休息，让GPU冷却
            time.sleep(2)
    
    # 翻译完成，单独更新批次信息
    perfmonitor.update_translation_stats({
        "current_batch": total_large_batches,
        "total_batches": total_large_batches
    })
    
    # 最终更新总体进度信息
    final_elapsed_time = time.time() - start_time
    perfmonitor.update_translation_stats({
        "processed_texts": processed_texts_global,
        "total_texts": total_texts_to_translate,
        "avg_time_per_text": final_elapsed_time / processed_texts_global if processed_texts_global > 0 else 0,
        "total_time": final_elapsed_time,
        "remaining_time": 0,  # 已完成，剩余时间为0
        "progress_percent": 100.0  # 已完成，进度为100%
    })
    
    logger.info(f"完成 {len(samples)} 条漏洞数据的中文翻译")
    logger.info(f"总翻译时间: {final_elapsed_time:.1f}秒, 平均每条: {final_elapsed_time/processed_texts_global:.3f}秒")
    return vulnerabilities

def process_nvd_vulnerabilities():
    """
    处理NVD漏洞数据
    
    Returns:
        list: 处理后的NVD漏洞数据
    """
    # 获取最近10年的NVD数据
    current_year = datetime.now().year
    nvd_vulnerabilities = fetch_nvd_data((current_year - 10, current_year))
    
    # 丰富数据
    return nvd_vulnerabilities

def process_cve_vulnerabilities():
    """
    处理CVE漏洞数据
    
    Returns:
        list: 处理后的CVE漏洞数据
    """
    # 获取CVE MITRE数据
    cve_vulnerabilities = fetch_cve_mitre_data()
    
    # 丰富数据
    return cve_vulnerabilities

def process_cnnvd_vulnerabilities():
    """
    处理CNNVD漏洞数据（模拟）
    
    Returns:
        list: 处理后的CNNVD漏洞数据
    """
    # 由于无法直接访问CNNVD数据，这里创建一些中文漏洞数据作为示例
    cnnvd_vulnerabilities = []
    
    # 模拟数据
    for i in range(1000):
        vuln_id = f"CNNVD-{(datetime.now().year - random.randint(0, 5)):04d}{random.randint(1, 12):02d}-{random.randint(1, 9999):04d}"
        risk_levels = ["高危", "中危", "低危"]
        vuln_types = ["缓冲区溢出", "跨站脚本", "SQL注入", "权限提升", "拒绝服务", "信息泄露", "命令注入"]
        
        vulnerability = {
            'id': vuln_id,
            'risk_level': random.choice(risk_levels),
            'type': random.choice(vuln_types),
            'function': f"function_{vuln_id.lower().replace('-', '_')}",
            'description': f"这是一个{random.choice(risk_levels)}级别的{random.choice(vuln_types)}漏洞，可能导致系统不稳定或安全问题。",
            'details': f"攻击者可能通过特定的输入触发此漏洞，导致系统出现异常行为。受影响的组件在处理特定输入时没有进行充分的验证和过滤。",
            'affected_versions': f"受影响的版本包括 v1.0-v{random.randint(1, 5)}.{random.randint(0, 9)}",
            'cve_reference': "",
            'discovered_date': f"{(datetime.now().year - random.randint(0, 5)):04d}-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}",
            'source': 'CNNVD模拟数据'
        }
        
        cnnvd_vulnerabilities.append(vulnerability)
    
    return cnnvd_vulnerabilities

def save_vulnerabilities(vulnerabilities, output_file=OUTPUT_FILE):
    """
    保存漏洞数据到JSON文件
    
    Args:
        vulnerabilities: 漏洞数据列表
        output_file: 输出文件路径
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({"vulnerabilities": vulnerabilities}, f, ensure_ascii=False, indent=2)
        logger.info(f"成功保存 {len(vulnerabilities)} 条漏洞数据到 {output_file}")
    except Exception as e:
        logger.error(f"保存漏洞数据时出错: {str(e)}")

def merge_vulnerabilities(nvd_data, cve_data, cnnvd_data):
    """
    合并来自不同来源的漏洞数据，去除重复项
    
    Args:
        nvd_data: NVD漏洞数据
        cve_data: CVE漏洞数据
        cnnvd_data: CNNVD漏洞数据
    
    Returns:
        list: 合并后的漏洞数据
    """
    all_data = nvd_data + cnnvd_data
    
    # 使用CVE数据补充NVD中没有的漏洞
    nvd_cve_ids = set(vuln['id'] for vuln in nvd_data)
    for vuln in cve_data:
        if vuln['id'] not in nvd_cve_ids:
            all_data.append(vuln)
    
    logger.info(f"合并后共有 {len(all_data)} 条漏洞数据")
    return all_data

# 增强翻译缓存
class TranslationCache:
    """翻译缓存类，用于存储和读取翻译结果"""
    
    def __init__(self, cache_file="translation_cache.pkl"):
        self.cache_file = os.path.join(DATA_DIR, cache_file)
        self.cache = {}
        self.pending_updates = 0
        self.max_pending_updates = 50  # 累积50个更新后保存
        self.load_cache()
    
    def load_cache(self):
        """从文件加载缓存"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    self.cache = pickle.load(f)
                logger.info(f"已加载 {len(self.cache)} 条翻译缓存")
            except Exception as e:
                logger.warning(f"加载翻译缓存失败: {str(e)}")
                self.cache = {}
    
    def save_cache(self, force=False):
        """保存缓存到文件"""
        if not force and self.pending_updates < self.max_pending_updates:
            return
            
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
            logger.info(f"已保存 {len(self.cache)} 条翻译缓存")
            self.pending_updates = 0
        except Exception as e:
            logger.warning(f"保存翻译缓存失败: {str(e)}")
    
    def get(self, text):
        """获取缓存的翻译结果"""
        return self.cache.get(text)
    
    def get_batch(self, texts):
        """批量获取缓存的翻译结果"""
        return [self.cache.get(text) for text in texts]
    
    def put(self, text, translation):
        """添加翻译结果到缓存"""
        self.cache[text] = translation
        self.pending_updates += 1
        if self.pending_updates >= self.max_pending_updates:
            self.save_cache()
    
    def put_batch(self, texts, translations):
        """批量添加翻译结果到缓存"""
        for text, translation in zip(texts, translations):
            if text and translation and text != translation:
                self.cache[text] = translation
                self.pending_updates += 1
        
        if self.pending_updates >= self.max_pending_updates:
            self.save_cache()

# 创建翻译缓存实例
translation_cache = TranslationCache()

# 使用缓存的翻译函数
def cached_translate(text):
    """使用缓存机制的翻译函数"""
    if not text:
        return text
    
    # 检查缓存
    cached_result = translation_cache.get(text)
    if cached_result:
        return cached_result
    
    # 没有缓存结果，执行翻译
    result = translate_to_chinese(text)
    
    # 存入缓存
    if result != text:  # 只缓存真正翻译了的结果
        translation_cache.put(text, result)
    
    return result

# 批量使用缓存的翻译函数
def cached_batch_translate(texts):
    """使用缓存机制的批量翻译函数"""
    if not texts:
        return []
    
    # 检查哪些文本需要翻译
    cached_results = translation_cache.get_batch(texts)
    needs_translation = []
    needs_translation_indices = []
    
    for i, (text, cached) in enumerate(zip(texts, cached_results)):
        if text and not cached:
            needs_translation.append(text)
            needs_translation_indices.append(i)
    
    # 如果所有文本都已缓存，直接返回缓存结果
    if not needs_translation:
        return [result if result else text for text, result in zip(texts, cached_results)]
    
    # 翻译未缓存的文本
    translated = batch_translate_to_chinese(needs_translation)
    
    # 更新缓存
    translation_cache.put_batch(needs_translation, translated)
    
    # 构建最终结果
    results = []
    translated_idx = 0
    
    for i, (text, cached) in enumerate(zip(texts, cached_results)):
        if i in needs_translation_indices:
            results.append(translated[translated_idx])
            translated_idx += 1
        else:
            results.append(cached if cached else text)
    
    return results

# 添加内存监控和释放函数
def check_and_free_memory(threshold_percent=90, gpu_threshold_percent=90):
    """
    检查系统内存和GPU内存使用情况，如果超过阈值则尝试释放资源
    
    Args:
        threshold_percent: 系统内存占用百分比阈值
        gpu_threshold_percent: GPU内存占用百分比阈值
        
    Returns:
        bool: 是否需要暂停或降低批处理大小
    """
    import gc
    
    needs_action = False
    
    # 检查系统内存
    try:
        mem_info = psutil.virtual_memory()
        mem_percent = mem_info.percent
        
        if mem_percent > threshold_percent:
            logger.warning(f"系统内存占用过高: {mem_percent}%，触发内存释放")
            needs_action = True
            
            # 强制执行垃圾回收
            gc.collect()
            
            # 如果有Torch，清理缓存
            if 'torch' in sys.modules:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # 检查释放后的内存情况
            mem_after = psutil.virtual_memory().percent
            logger.info(f"内存释放后: {mem_after}%（释放了 {mem_percent - mem_after}%）")
    except Exception as e:
        logger.error(f"检查系统内存时出错: {str(e)}")
    
    # 检查GPU内存
    try:
        if 'torch' in sys.modules:
            import torch
            if torch.cuda.is_available():
                # 获取当前GPU内存使用情况
                gpu_info = get_gpu_info()
                
                if "memory_percent" in gpu_info and gpu_info["memory_percent"] > gpu_threshold_percent:
                    logger.warning(f"GPU内存占用过高: {gpu_info['memory_percent']:.1f}%，触发内存释放")
                    needs_action = True
                    
                    # 清理PyTorch缓存
                    torch.cuda.empty_cache()
                    
                    # 检查释放后的GPU情况
                    gpu_after = get_gpu_info()
                    if "memory_percent" in gpu_after:
                        logger.info(f"GPU内存释放后: {gpu_after['memory_percent']:.1f}%（释放了 {gpu_info['memory_percent'] - gpu_after['memory_percent']:.1f}%）")
    except Exception as e:
        logger.error(f"检查GPU内存时出错: {str(e)}")
    
    return needs_action

def main():
    """主函数"""
    logger.info("开始获取CVE漏洞数据")
    
    # 检查依赖
    dependencies_ok = check_dependencies()
    if not dependencies_ok:
        logger.warning("部分依赖未安装，性能监控功能可能不可用")
    
    # 根据配置决定是否启动性能监控
    if PERFORMANCE_MONITOR["enabled"]:
        # 启动Web性能监控服务（在单独的线程中）
        def start_web_monitor_thread():
            try:
                # 使用全局配置的参数
                host = PERFORMANCE_MONITOR["host"]
                port = PERFORMANCE_MONITOR["port"]
                interval = PERFORMANCE_MONITOR["update_interval"]
                
                logger.info(f"启动Web性能监控服务，访问 http://{host}:{port} 查看性能数据")
                perfmonitor.start_web_monitor(host=host, port=port, interval=interval)
            except Exception as e:
                logger.error(f"启动Web性能监控服务失败: {str(e)}")
        
        # 创建并启动Web监控线程
        web_monitor_thread = threading.Thread(target=start_web_monitor_thread)
        web_monitor_thread.daemon = True  # 设置为守护线程，主线程结束时自动终止
        web_monitor_thread.start()
        
        # 等待Web服务器启动
        time.sleep(1)
    
    # 定期清理内存的监控线程
    def memory_monitor_thread():
        while True:
            try:
                # 定期检查并释放内存
                check_and_free_memory(threshold_percent=70, gpu_threshold_percent=70)
                time.sleep(30)  # 每30秒检查一次
            except Exception as e:
                logger.error(f"内存监控线程错误: {str(e)}")
                time.sleep(60)  # 出错后等待时间更长
    
    # 启动内存监控线程
    mem_thread = threading.Thread(target=memory_monitor_thread)
    mem_thread.daemon = True
    mem_thread.start()
    
    # 只处理CVE漏洞数据
    cve_vulnerabilities = process_cve_vulnerabilities()
    logger.info(f"获取到 {len(cve_vulnerabilities)} 条CVE漏洞数据")
    
    # 对所有数据进行中文翻译
    enriched_vulnerabilities = enrich_data_with_chinese_translation(cve_vulnerabilities)
    
    # 保存数据到JSON文件
    output_file = os.path.join(DATA_DIR, "cve_vulnerabilities.json")
    save_vulnerabilities(enriched_vulnerabilities, output_file)
    
    logger.info("CVE漏洞数据获取和翻译完成")
    
    # 如果性能监控已启用，通知用户Web服务仍在运行
    if PERFORMANCE_MONITOR["enabled"]:
        print(f"\n性能监控Web服务器仍在运行，访问 http://{PERFORMANCE_MONITOR['host']}:{PERFORMANCE_MONITOR['port']} 查看性能数据。")
        print("按Ctrl+C终止程序。")
        
        # 保持主线程运行，直到用户按下Ctrl+C
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\n程序已终止。")

if __name__ == "__main__":
    main() 